{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from options import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from vectorizer import init_tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarigian/miniconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets = pd.read_table(DATA_PATH+POS_TWEETS_FILE, names=['tweet','sentiment'])\n",
    "pos_tweets['sentiment'] = 'pos'\n",
    "neg_tweets = pd.read_table(DATA_PATH+NEG_TWEETS_FILE ,names=['tweet','sentiment'])\n",
    "neg_tweets['sentiment'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pos_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neg_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive tweets shape:  (97902, 2)\n",
      "negative tweets shape:  (99068, 2)\n"
     ]
    }
   ],
   "source": [
    "print('positive tweets shape: ',pos_tweets.shape)\n",
    "print('negative tweets shape: ',neg_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196970, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.concat([pos_tweets, neg_tweets], axis=0)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  sea doo pro sea scooter ( sports with the port...        NaN\n",
       "1  <user> shucks well i work all week so now i ca...        NaN\n",
       "2            i cant stay away from bug thats my baby        NaN\n",
       "3  <user> no ma'am ! ! ! lol im perfectly fine an...        NaN\n",
       "4  whenever i fall asleep watching the tv , i alw...        NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets = pd.read_table(DATA_PATH+TEST_TWEETS_FILE, names=['tweet','sentiment'])\n",
    "test_tweets['tweet'] = test_tweets.apply(lambda tweet: remove_tweet_id(tweet['tweet']), axis=1)\n",
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tweets['tagged'] = tweets.apply(lambda tweet: pos_tag(tweet['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Settings:\n",
      "\n",
      "fhashtag :\t True\n",
      "fuser :\t True\n",
      "fduplicates :\t True\n",
      "frepeated_chars :\t True\n",
      "furl :\t True\n",
      "fdigits :\t True\n",
      "fsmall_words :\t True\n",
      "save :\t True\n",
      "fpunctuation :\t True\n",
      "-\n",
      "\n",
      "Tweets Preprocessing for the Training set started\n",
      "\n",
      "\n",
      "Tweets have been successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "tweets = preprocessing(tweets,train=True, params=preprocessing_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets final representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dunno justin read mention not only justin and ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>because your logic dumb won even crop out your...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just put casper box looved the battle crakkbitch</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks sir don trip lil mama just keep doin thang</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting brother tmr the bestest birthday gift...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yay lifecompleted tweet facebook let know please</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dnextalbumtitle feel for you rollercoaster lif...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>workin hard hardly workin hardee with future c...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saw replying bit</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this were belong</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>andd cheer nationals</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>send invitation shop line here you will find e...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>just woke finna church</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>agreed more days left tho</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>monet with katemelo</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>like damm lexis got lot say when twitter lol</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>grateful today for dream fulfilled heart full ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>home affairs shall later</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>barca bout beat real madrid saturday doe</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lot parts asia especially rats that live the c...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wasn even sleeping shut cho ole back sleep loo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>have the worlds best dad</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>jaeyay werna meeting khatam hojaeygi baaqi but...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>one doubts that ability</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>check tweet pic out that was the outfit before...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>just got mid term and impressed happy</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>summer days work from ish home shower eat out ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lol noo food friend</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>millionbritneyfan and tweet</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>but seriously though called vanity fairest</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178453</th>\n",
       "      <td>omg well long she comes wales soon ish</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178454</th>\n",
       "      <td>custom picture frame poster frame wide complet...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178455</th>\n",
       "      <td>wyatt yrs old just said sex the airr omg love ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178456</th>\n",
       "      <td>fijne avond allemaal enjoy the rain</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178457</th>\n",
       "      <td>aww missed the water gun fightt</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178458</th>\n",
       "      <td>zebra mil receipt receipt paper mil roll roll ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178459</th>\n",
       "      <td>want this feeling end asap</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178460</th>\n",
       "      <td>miss you already aswell see you soon though</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178461</th>\n",
       "      <td>ahh that suc cant come live</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178462</th>\n",
       "      <td>yesseu yesseu tried earlier and wasn working a...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178463</th>\n",
       "      <td>hate papers almost much mushrooms ick</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178464</th>\n",
       "      <td>how become london bus driver the insider guide...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178465</th>\n",
       "      <td>prod plastic script style numbers pack this it...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178466</th>\n",
       "      <td>can take any longer thought that were stronger...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178467</th>\n",
       "      <td>debbi fields great american desserts hardcover...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178468</th>\n",
       "      <td>the right side heads fell</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178469</th>\n",
       "      <td>life surrounded tedious stuffs superboring now</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178470</th>\n",
       "      <td>bedknobs and broomsticks dvd academy award win...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178471</th>\n",
       "      <td>its not problem follow its problem that you wi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178472</th>\n",
       "      <td>last status update from the wall martyr siachi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178473</th>\n",
       "      <td>rip meridith will miss meridith marks obituary...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178474</th>\n",
       "      <td>ofcos honey but not meeting you imissyousososo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178475</th>\n",
       "      <td>bedtime have fun the game tomorrow all you luc...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178476</th>\n",
       "      <td>went through pictures phone found this miss him</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178477</th>\n",
       "      <td>everyday negotiation navigating the hidden age...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178478</th>\n",
       "      <td>can wait fake tan tonight hate being pale</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178479</th>\n",
       "      <td>darling lost internet connection and seems not...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178480</th>\n",
       "      <td>kanguru defender basic usb flash drive kdfb bl...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178481</th>\n",
       "      <td>rizan sad now</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178482</th>\n",
       "      <td>text back yea mad</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178483 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet sentiment\n",
       "0       dunno justin read mention not only justin and ...       pos\n",
       "1       because your logic dumb won even crop out your...       pos\n",
       "2        just put casper box looved the battle crakkbitch       pos\n",
       "3       thanks sir don trip lil mama just keep doin thang       pos\n",
       "4       visiting brother tmr the bestest birthday gift...       pos\n",
       "5        yay lifecompleted tweet facebook let know please       pos\n",
       "6       dnextalbumtitle feel for you rollercoaster lif...       pos\n",
       "7       workin hard hardly workin hardee with future c...       pos\n",
       "8                                        saw replying bit       pos\n",
       "9                                        this were belong       pos\n",
       "10                                   andd cheer nationals       pos\n",
       "11      send invitation shop line here you will find e...       pos\n",
       "12                                 just woke finna church       pos\n",
       "13                              agreed more days left tho       pos\n",
       "14                                    monet with katemelo       pos\n",
       "15           like damm lexis got lot say when twitter lol       pos\n",
       "16      grateful today for dream fulfilled heart full ...       pos\n",
       "17                               home affairs shall later       pos\n",
       "18               barca bout beat real madrid saturday doe       pos\n",
       "19      lot parts asia especially rats that live the c...       pos\n",
       "20      wasn even sleeping shut cho ole back sleep loo...       pos\n",
       "21                               have the worlds best dad       pos\n",
       "22      jaeyay werna meeting khatam hojaeygi baaqi but...       pos\n",
       "23                                one doubts that ability       pos\n",
       "24      check tweet pic out that was the outfit before...       pos\n",
       "25                  just got mid term and impressed happy       pos\n",
       "26      summer days work from ish home shower eat out ...       pos\n",
       "27                                    lol noo food friend       pos\n",
       "28                            millionbritneyfan and tweet       pos\n",
       "29             but seriously though called vanity fairest       pos\n",
       "...                                                   ...       ...\n",
       "178453             omg well long she comes wales soon ish       neg\n",
       "178454  custom picture frame poster frame wide complet...       neg\n",
       "178455  wyatt yrs old just said sex the airr omg love ...       neg\n",
       "178456                fijne avond allemaal enjoy the rain       neg\n",
       "178457                    aww missed the water gun fightt       neg\n",
       "178458  zebra mil receipt receipt paper mil roll roll ...       neg\n",
       "178459                         want this feeling end asap       neg\n",
       "178460        miss you already aswell see you soon though       neg\n",
       "178461                        ahh that suc cant come live       neg\n",
       "178462  yesseu yesseu tried earlier and wasn working a...       neg\n",
       "178463              hate papers almost much mushrooms ick       neg\n",
       "178464  how become london bus driver the insider guide...       neg\n",
       "178465  prod plastic script style numbers pack this it...       neg\n",
       "178466  can take any longer thought that were stronger...       neg\n",
       "178467  debbi fields great american desserts hardcover...       neg\n",
       "178468                          the right side heads fell       neg\n",
       "178469     life surrounded tedious stuffs superboring now       neg\n",
       "178470  bedknobs and broomsticks dvd academy award win...       neg\n",
       "178471  its not problem follow its problem that you wi...       neg\n",
       "178472  last status update from the wall martyr siachi...       neg\n",
       "178473  rip meridith will miss meridith marks obituary...       neg\n",
       "178474  ofcos honey but not meeting you imissyousososo...       neg\n",
       "178475  bedtime have fun the game tomorrow all you luc...       neg\n",
       "178476    went through pictures phone found this miss him       neg\n",
       "178477  everyday negotiation navigating the hidden age...       neg\n",
       "178478          can wait fake tan tonight hate being pale       neg\n",
       "178479  darling lost internet connection and seems not...       neg\n",
       "178480  kanguru defender basic usb flash drive kdfb bl...       neg\n",
       "178481                                      rizan sad now       neg\n",
       "178482                                  text back yea mad       neg\n",
       "\n",
       "[178483 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.isnull(tweets).any(1).nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frequencies TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize bag of words (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf Vectorizer settings\n",
      "\n",
      "use_idf :\t True\n",
      "min_df :\t 5\n",
      "number_of_stopwords :\t 153\n",
      "max_df :\t 0.8\n",
      "ngram_range :\t (1, 1)\n",
      "tokenizer :\t True\n",
      "max_features :\t 5000\n",
      "sublinear_tf :\t True\n",
      "-\n",
      "\n",
      "stopwords:\n",
      " frozenset({'nevertheless', 'via', 'though', 'which', 'whoever', 'anyone', 'anyhow', 'did', 'because', 'there', 'un', 'name', 'never', 'wherein', 'eight', 'some', 'not', 'fill', 'throughout', 'yet', 'about', 'from', 'ourselves', 'toward', 'whatever', 'she', 'someone', 'll', 'seemed', 'nine', 'somehow', 'be', 'does', 'etc', 'else', 'every', 'least', 'always', 'fifteen', 'ours', 'own', 'having', 'isn', 'last', 'third', 'him', 'mightn', 'enough', 'very', 'afterwards', 'fire', 'didn', 'whereby', 'nor', 'been', 'our', 'each', 'o', 'wherever', 'a', 'now', 'her', 'you', 'other', 'around', 'still', 'cant', 'below', 'have', 'mostly', 'rather', 'bottom', 've', 'ma', 'ain', 'moreover', 'alone', 'although', 'has', 'hereby', 'former', 'even', 'per', 'sincere', 'indeed', 'something', 'am', 'nobody', 'here', 'whose', 'namely', 'whereupon', 'made', 'somewhere', 'hereupon', 'before', 'himself', 'cry', 'more', 'without', 'many', 'anything', 'another', 'until', 'call', 'is', 'wasn', 'everyone', 'become', 'them', 'whereafter', 'beyond', 'myself', 'whether', 'also', 'ie', 'same', 'few', 'seem', 'give', 'whenever', 'detail', 'your', 'put', 'whole', 'into', 'mine', 'than', 'thick', 'formerly', 'down', 'noone', 'us', 'latterly', 'can', 'were', 'through', 'just', 'if', 'amoungst', 'twenty', 'four', 'whence', 'fifty', 'cannot', 'should', 'further', 'my', 'theirs', 'nowhere', 'most', 'me', 'seeming', 'twelve', 'y', 'as', 'front', 'no', 'those', 'both', 'his', 'describe', 'top', 'almost', 'by', 'becomes', 'will', 'across', 'off', 'hence', 'its', 'had', 'serious', 'again', 'five', 'and', 'first', 'why', 'often', 'system', 'between', 'once', 'besides', 'others', 'in', 'all', 'where', 'thereafter', 'could', 'don', 'they', 'so', 'con', 'weren', 'shan', 'sixty', 'together', 'any', 'two', 'full', 'anyway', 'hasnt', 'find', 'sometimes', 'ever', 'shouldn', 'less', 'onto', 'with', 'empty', 'during', 'beforehand', 'ltd', 'it', 'but', 'found', 'themselves', 'becoming', 'over', 'hundred', 'latter', 'an', 'then', 'doesn', 'of', 'well', 'seems', 'therein', 's', 'except', 'out', 'against', 're', 'became', 'among', 'see', 'for', 'he', 'beside', 'hadn', 'along', 'co', 'the', 'thence', 'after', 't', 'eg', 'back', 'would', 'thereby', 'done', 'take', 'won', 'might', 'under', 'd', 'go', 'interest', 'aren', 'six', 'herself', 'nothing', 'only', 'everything', 'how', 'must', 'haven', 'amongst', 'thus', 'keep', 'everywhere', 'thin', 'eleven', 'however', 'much', 'otherwise', 'hereafter', 'up', 'part', 'please', 'to', 'couldnt', 'next', 'elsewhere', 'towards', 'hasn', 'their', 'above', 'i', 'upon', 'when', 'being', 'yours', 'at', 'several', 'due', 'either', 'whereas', 'amount', 'move', 'this', 'whither', 'couldn', 'within', 'on', 'wouldn', 'who', 'are', 'perhaps', 'thru', 'while', 'thereupon', 'needn', 'sometime', 'these', 'three', 'none', 'that', 'yourself', 'itself', 'behind', 'already', 'mustn', 'm', 'inc', 'therefore', 'whom', 'side', 'such', 'we', 'one', 'was', 'too', 'what', 'get', 'or', 'since', 'ten', 'neither', 'forty', 'may', 'meanwhile', 'yourselves', 'do', 'herein', 'bill', 'hers', 'doing', 'de', 'anywhere', 'mill', 'show'})\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = init_tfidf_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### polynomial expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (simple training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets['tweet'], tweets['sentiment'], test_size=split_params['test_size'], random_state=split_params['random_state'])\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)\n",
    "#shape: (number_of_tweets, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbclf = MultinomialNB()\n",
    "nbclf.fit(tfidf_train_vectors, y_train)\n",
    "prediction_bayes = nbclf.predict(tfidf_test_vectors)\n",
    "print(prediction_bayes.shape)\n",
    "print(classification_report(y_test, prediction_bayes))\n",
    "print('score: ',accuracy_score(y_test,prediction_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice algorithm because it runs in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# forest = RandomForestClassifier(n_estimators=100,max_depth=100,n_jobs=-1,random_state=4)\n",
    "# forest.fit(tfidf_train_vectors, y_train)\n",
    "# y_pred_forest = forest.predict(tfidf_test_vectors)\n",
    "\n",
    "# print(classification_report(y_test, y_pred_forest))\n",
    "# print('score: ',accuracy_score(y_test,y_pred_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classifier_linear = svm.SVC(kernel='linear')\n",
    "# classifier_linear.fit(tfidf_train_vectors, y_train)\n",
    "# prediction_linear = classifier_linear.predict(tfidf_test_vectors)\n",
    "\n",
    "# print(classification_report(y_test, prediction_linear))\n",
    "# print('score: ',accuracy_score(y_test,prediction_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top k most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(topk_most_important_features(tfidf_vectorizer, nbclf, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_most_informative_features(tfidf_vectorizer, nbclf, n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K fold Cross validation & Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf_train_vectors = tfidf_vectorizer.fit_transform(tweets['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to do a for loop to find best alpha param (when get access to server)\n",
    "# naivebayesclf = MultinomialNB()\n",
    "# avg_test_accuracy, cv_bayes = cross_validation(naivebayesclf , tweets.shape[0], tfidf_train_vectors, tweets['sentiment'], n_folds=kfold['naive_bayes'])\n",
    "# print('avg score: ',avg_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests (Model Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to do a for loop to find best parameters (when get access to server)\n",
    "# forest_clf = RandomForestClassifier(n_estimators=100,max_depth=100,n_jobs=-1,random_state=4)\n",
    "# avg_test_accuracy, cv_forest = cross_validation(forest_clf , tweets.shape[0], tfidf_train_vectors, tweets['sentiment'], n_folds=kfold['random_forest'])\n",
    "# print('avg score: ',avg_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_learning_curve(naivebayesclf, 'Learning Curve - Naive Bayes', tfidf_train_vectors, tweets['sentiment'], cv=cv_bayes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot_learning_curve(forest_clf, 'Learning Curve - Random Forest', tfidf_train_vectors, tweets['sentiment'], cv=cv_forest)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.vocabulary_\n",
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tweets = preprocessing(test_tweets,train=False, params=preprocessing_params)\n",
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_clf = MultinomialNB()\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(tweets['tweet'])\n",
    "test_data = tfidf_vectorizer.transform(test_tweets['tweet'])\n",
    "final_clf.fit(tfidf_train_vectors, tweets['sentiment'])\n",
    "pred = final_clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_csv_submission(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code & Useful stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.vocabulary_['follow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    " \n",
    "print(\"Stem %s: %s\" % (\"studying\", stemmer.stem(\"studying\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.load(DATA_PATH+'embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
